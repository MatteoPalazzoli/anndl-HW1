{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network for Image Recognition\n",
    "\n",
    "\"BlendedLeaf\" - AN2DL homework 1\n",
    "## Team\n",
    "\"Augmented Tensorflop\"\n",
    "- Daniele Civati\n",
    "- Matteo Palazzoli\n",
    "- Francesco Panebianco\n",
    "\n",
    "## Imports & suppress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from math import floor\n",
    "\n",
    "#----------------------------------------------------------\n",
    "# suppress warnings\n",
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now import the tensorflow addons, that will be useful for our custom optimizer. The exported model will have the same architecture, weights and other parameters but Adam as optimizer to avoid import error for unavaiable libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data retrieval and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path used in Kaggle\n",
    "path_to_dataset = '../input/homework1/training_data_final/'\n",
    "\n",
    "# let's get a sample to get the dimensions (that are 96x96)\n",
    "image_representative = PIL.Image.open(path_to_dataset + 'Species1/00000.jpg').convert('RGB')\n",
    "img_width = image_representative.size[0]\n",
    "img_height = image_representative.size[1]\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# get the files and turn the information into lists\n",
    "for class_num in range(8):\n",
    "    for root, dirs, files in os.walk(path_to_dataset + 'Species' + str(class_num + 1) + '/'):\n",
    "        for file in files:\n",
    "            file_path = root + file\n",
    "            image=tf.keras.preprocessing.image.load_img(file_path, color_mode='rgb', \n",
    "            target_size= (img_width, img_height))\n",
    "            image=np.array(image)\n",
    "        \n",
    "            # Add data point and label to the list\n",
    "            data.append(image)\n",
    "            labels.append(class_num)\n",
    "\n",
    "# convert to numpy\n",
    "data = np.array(data)\n",
    "\n",
    "# preprocess input for VGG16 and convert labels to categorical\n",
    "data = keras.applications.vgg16.preprocess_input(data)\n",
    "\n",
    "ind_labels = np.array(labels)\n",
    "\n",
    "# convert the labels to one-hot-encoding\n",
    "labels = keras.utils.to_categorical(ind_labels, num_classes=8)\n",
    "\n",
    "print('Data shape: {}'.format(data.shape))\n",
    "print('Labels shape: {}'.format(labels.shape))\n",
    "\n",
    "# decide if there will be a separate test set or not\n",
    "test = True\n",
    "if(test):\n",
    "    X_trainval, X_test, y_trainval, y_test = train_test_split(data, labels, test_size=0.15, shuffle=True, stratify=labels)\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_trainval, y_trainval, test_size=0.08 / 0.85, shuffle=True, stratify=y_trainval)\n",
    "else:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(data, labels, test_size=0.08, shuffle=True, stratify=labels)\n",
    "print('\\n\\nTraining Set:')\n",
    "print('Data shape: {}'.format(X_train.shape))\n",
    "print('Labels shape: {}'.format(y_train.shape))\n",
    "print('\\nValidation Set:')\n",
    "print('Data shape: {}'.format(X_valid.shape))\n",
    "print('Labels shape: {}'.format(y_valid.shape))\n",
    "if(test):\n",
    "    print('\\nTest Set:')\n",
    "    print('Data shape: {}'.format(X_test.shape))\n",
    "    print('Labels shape: {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's duplicate the data for the next augmentation techniques,\n",
    "# because the CutMix and MixUp utilities take 2 inputs to give one output.\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_ds_one = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "train_ds_two = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    .shuffle(BATCH_SIZE * 100)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "# conversion to tf.data.Dataset type\n",
    "train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(BATCH_SIZE)\n",
    "if(test):\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CutMix Augmentation Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUTMIX utility function\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n",
    "    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n",
    "    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n",
    "    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CUTMIX augmentation technique from Keras documentation\n",
    "IMG_SIZE = 96\n",
    "\n",
    "# the function is decorated with this annotation for performance improvement\n",
    "@tf.function\n",
    "def get_box(lambda_value):\n",
    "    cut_rat = tf.math.sqrt(1.0 - lambda_value)\n",
    "    cut_w = IMG_SIZE * cut_rat  # rw\n",
    "    cut_w = tf.cast(cut_w, tf.int32)\n",
    "    cut_h = IMG_SIZE * cut_rat  # rh\n",
    "    cut_h = tf.cast(cut_h, tf.int32)\n",
    "    cut_x = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # rx\n",
    "    cut_y = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # ry\n",
    "    boundaryx1 = tf.clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)\n",
    "    boundaryy1 = tf.clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)\n",
    "    bbx2 = tf.clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)\n",
    "    bby2 = tf.clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)\n",
    "    target_h = bby2 - boundaryy1\n",
    "    if target_h == 0:\n",
    "        target_h += 1\n",
    "    target_w = bbx2 - boundaryx1\n",
    "    if target_w == 0:\n",
    "        target_w += 1\n",
    "    return boundaryx1, boundaryy1, target_h, target_w\n",
    "\n",
    "@tf.function\n",
    "def cutmix(train_ds_one, train_ds_two):\n",
    "    image1, label1 = train_ds_one\n",
    "    image2, label2 = train_ds_two\n",
    "\n",
    "    alpha = [0.25]\n",
    "    beta = [0.25]\n",
    "\n",
    "    # Get a sample from the Beta distribution\n",
    "    lambda_value = sample_beta_distribution(1, alpha, beta)\n",
    "\n",
    "    # Define Lambda\n",
    "    lambda_value = lambda_value[0][0]\n",
    "\n",
    "    # Get the bounding box offsets, heights and widths\n",
    "    boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)\n",
    "\n",
    "    # Get a patch from the second image (`image2`)\n",
    "    crop2 = tf.image.crop_to_bounding_box(\n",
    "        image2, boundaryy1, boundaryx1, target_h, target_w\n",
    "    )\n",
    "    # Pad the `image2` patch (`crop2`) with the same offset\n",
    "    image2 = tf.image.pad_to_bounding_box(\n",
    "        crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n",
    "    )\n",
    "    # Get a patch from the first image (`image1`)\n",
    "    crop1 = tf.image.crop_to_bounding_box(\n",
    "        image1, boundaryy1, boundaryx1, target_h, target_w\n",
    "    )\n",
    "    # Pad the `image1` patch (`crop1`) with the same offset\n",
    "    img1 = tf.image.pad_to_bounding_box(\n",
    "        crop1, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n",
    "    )\n",
    "\n",
    "    # Modify the first image by subtracting the patch from `image1`\n",
    "    # (before applying the `image2` patch)\n",
    "    image1 = image1 - img1\n",
    "    # Add the modified `image1` and `image2`  together to get the CutMix image\n",
    "    image = image1 + image2\n",
    "\n",
    "    # Adjust Lambda in accordance to the pixel ration\n",
    "    lambda_value = 1 - (target_w * target_h) / (IMG_SIZE * IMG_SIZE)\n",
    "    lambda_value = tf.cast(lambda_value, tf.float32)\n",
    "\n",
    "    # Combine the labels of both images\n",
    "    label = lambda_value * label1 + (1 - lambda_value) * label2\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply our `cutmix` utility\n",
    "train_ds = train_ds.map(\n",
    "    lambda ds_one, ds_two: cutmix(ds_one, ds_two), num_parallel_calls=AUTO\n",
    ")\n",
    "\n",
    "class_names = [\"Species1\", \"Species2\", \"Species3\", \"Species4\", \"Species5\", \"Species6\", \"Species7\", \"Species8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's preview 9 samples from the dataset\n",
    "image_batch, label_batch = next(iter(train_ds))\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.title(class_names[np.argmax(label_batch[i])])\n",
    "    plt.imshow(image_batch[i]) \n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# the images below are very strange because of VGG16's preprocessing:\n",
    "# The images are converted from RGB to BGR, \n",
    "# then each color channel is zero-centered with respect to the ImageNet dataset, without scaling. \n",
    "# We can still see the \"boxes\" generated by CutMix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_vector = compute_class_weight(class_weight='balanced', classes=[*range(8)], y=ind_labels)\n",
    "weight_vector[0] = weight_vector[0] * 2.0\n",
    "weight_vector[7] = weight_vector[7] * 1.7\n",
    "class_weights = {i : weight_vector[i] for i in range(8)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "With VGG16 pretrained on ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = keras.applications.VGG16(include_top=False,\n",
    "                                           weights='imagenet',\n",
    "                                           input_shape=(img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model.trainable = True\n",
    "\n",
    "# model\n",
    "ft_model = keras.Sequential(name = 'VGG16_BlendedLeaf')\n",
    "ft_model.add(keras.layers.Input(shape=(img_width, img_height, 3)))\n",
    "ft_model.add(keras.layers.RandomFlip())\n",
    "ft_model.add(keras.layers.RandomTranslation(0.15, 0.15))\n",
    "ft_model.add(pretrained_model)\n",
    "ft_model.add(keras.layers.Flatten(name='flattener'))\n",
    "ft_model.add(keras.layers.Dropout(0.4))\n",
    "ft_model.add(keras.layers.Dense(256, activation='relu',\n",
    "    kernel_initializer = keras.initializers.HeUniform(), name='classifier'))\n",
    "ft_model.add(keras.layers.Dense(8, activation='softmax',\n",
    "    kernel_initializer = keras.initializers.GlorotUniform(), name='output'))\n",
    "\n",
    "# Freeze pretrained weights up to a certain limit\n",
    "freeze_layer_limit = 7\n",
    "\n",
    "# Freeze first layers\n",
    "for i, layer in enumerate(ft_model.get_layer('vgg16').layers[:freeze_layer_limit]):\n",
    "    layer.trainable=False\n",
    "for i, layer in enumerate(ft_model.get_layer('vgg16').layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "\n",
    "ft_model.compile(loss=keras.losses.CategoricalCrossentropy(), optimizer=tfa.optimizers.Yogi(1e-4), metrics='accuracy')\n",
    "ft_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "An early stopping callback is added to the process to limit overfitting, with a patience of 15.\n",
    "We perform the training on the partially unfrozen model because it gives better performance than plain Trasnfer Learning.\n",
    "#### Notes\n",
    "Please note: sometimes there's an unlucky initialization which prevents convergence. In that case just re-run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "ft_history = ft_model.fit(\n",
    "    x = train_ds,\n",
    "    batch_size = 32,\n",
    "    epochs = 200,\n",
    "    validation_data = valid_ds,\n",
    "    class_weight=class_weights,\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)]\n",
    ").history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set with the CNN\n",
    "# in case there's no test set, use validation set (but the results will be less representative)\n",
    "if(test):\n",
    "    testset = X_test\n",
    "    testlabels = y_test\n",
    "else:\n",
    "    testset = X_valid\n",
    "    testlabels = y_valid\n",
    "predictions = ft_model.predict(testset)\n",
    "predictions.shape\n",
    "label_classes = ['Species1', 'Species2', 'Species3', 'Species4', 'Species5', 'Species6', 'Species7', 'Species8']\n",
    "\n",
    "# Compute the confusion matrix\n",
    "true_ = np.argmax(testlabels, axis=-1)\n",
    "pred_ = np.argmax(predictions, axis=-1)\n",
    "cm = confusion_matrix(true_, pred_)\n",
    "cm_rel = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Compute the classification metrics\n",
    "accuracy = accuracy_score(true_, pred_)\n",
    "precision = precision_score(true_, pred_, average='macro')\n",
    "recall = recall_score(true_, pred_, average='macro')\n",
    "f1 = f1_score(true_, pred_, average='macro')\n",
    "print('Accuracy:',accuracy.round(4))\n",
    "print('Precision:',precision.round(4))\n",
    "print('Recall:',recall.round(4))\n",
    "print('F1:',f1.round(4))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cm_rel.T, xticklabels=label_classes, yticklabels=label_classes, annot=True, fmt='.2f')\n",
    "plt.xlabel('True labels')\n",
    "plt.ylabel('Predicted labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(ft_history['loss'], label='Training Loss', alpha=.3, color='#4D61E2', linestyle='--')\n",
    "plt.plot(ft_history['val_loss'], label='Validation Loss', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Categorical Crossentropy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(ft_history['accuracy'], label = 'Training Accuracy', alpha=.3, color='#4D61E2', linestyle='--')\n",
    "plt.plot(ft_history['val_accuracy'], label='Validation Accuracy', alpha=.8, color='#4D61E2')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.grid(alpha=.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model save & export\n",
    "We export the model with Adam optimizer, so it can run in an environment without tensorflow-addons installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.compile(loss=keras.losses.CategoricalCrossentropy(), optimizer=keras.optimizers.Adam(1e-4), metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "ft_model.save('VGG16_BlendedLef')\n",
    "\n",
    "!zip -r VGG16_BlendedLeaf.zip ./VGG16_BlendedLeaf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
